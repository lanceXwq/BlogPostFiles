<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Optimization Techniques in Scientific Computing (Part III)</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="optimization-techniques-in-scientific-computing-part-iii">Optimization Techniques in Scientific Computing (Part III)</h1>
<ul>
<li><a href="#optimization-techniques-in-scientific-computing-part-iii">Optimization Techniques in Scientific Computing (Part III)</a>
<ul>
<li><a href="#introduction-and-recap">Introduction and recap</a></li>
<li><a href="#run-code-on-a-gpu">Run code on a GPU</a>
<ul>
<li><a href="#first-attempt">First attempt</a></li>
<li><a href="#further-vectorization">Further vectorization</a></li>
<li><a href="#batch-matrix-multiplication">Batch matrix multiplication</a></li>
<li><a href="#final-boost">Final boost</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
<h2 id="introduction-and-recap">Introduction and recap</h2>
<p>In my <a href="https://labpresse.com/?s=Optimization+Techniques+in+Scientific+Computing">previous two blogs</a> for optimization techniques in scientific computing, I have talked about concepts such as vectorization and parallelism in the context of my single-molecule video simulation<sup class="footnote-ref"><a href="#fn1" id="fnref1">1</a></sup>, which can be mathematically formulated as calculating 3D array <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathnormal" style="margin-right: 0.22222em;">V</span></span></span></span></span> with <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>V</mi><mrow><mi>f</mi><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>n</mi></munder><mi>exp</mi><mo>⁡</mo><mo stretchy="false">[</mo><mo>−</mo><mo stretchy="false">(</mo><msubsup><mi>x</mi><mrow><mi>f</mi><mi>i</mi></mrow><mi>p</mi></msubsup><mo>−</mo><msub><mi>x</mi><mrow><mi>f</mi><mi>n</mi></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>−</mo><mo stretchy="false">(</mo><msubsup><mi>y</mi><mrow><mi>f</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>−</mo><msub><mi>y</mi><mrow><mi>f</mi><mi>n</mi></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">V_{fij}=\sum_n \exp[-(x^p_{fi}-x_{fn})^2-(y^p_{fj}-y_{fn})^2].</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.10764em;">f</span><span class="mord mathnormal mtight" style="margin-right: 0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.30001em; vertical-align: -1.25001em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.05001em;"><span class="" style="top: -1.89999em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.25001em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop">exp</span><span class="mopen">[</span><span class="mord">−</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7823em;"><span class="" style="top: -2.39869em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.10764em;">f</span><span class="mord mathnormal mtight">i</span></span></span></span><span class="" style="top: -3.18091em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.437416em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.15022em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.10764em;">f</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.21972em; vertical-align: -0.437416em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7823em;"><span class="" style="top: -2.39869em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.10764em;">f</span><span class="mord mathnormal mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="" style="top: -3.18091em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.437416em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.15022em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.10764em;">f</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">]</span><span class="mord">.</span></span></span></span></span></span> We started with <code>video_sim_v1</code></p>
<pre class=" language-julia"><code class="prism  language-julia"><span class="token keyword">function</span> video_sim_v1<span class="token punctuation">(</span>xᵖ<span class="token punctuation">,</span> yᵖ<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    <span class="token number">F</span> <span class="token operator">=</span> size<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    v <span class="token operator">=</span> Array<span class="token punctuation">{</span>eltype<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">}</span><span class="token punctuation">(</span>undef<span class="token punctuation">,</span> length<span class="token punctuation">(</span>xᵖ<span class="token punctuation">)</span><span class="token punctuation">,</span> length<span class="token punctuation">(</span>yᵖ<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">F</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> <span class="token number">f</span> in <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">F</span>
        PSFˣ <span class="token operator">=</span> exp<span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>xᵖ <span class="token punctuation">.</span><span class="token operator">-</span> Transpose<span class="token punctuation">(</span>view<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">f</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span>
        PSFʸ <span class="token operator">=</span> exp<span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>view<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">f</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">-</span> Transpose<span class="token punctuation">(</span>yᵖ<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span>
        v<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">f</span><span class="token punctuation">]</span> <span class="token operator">=</span> PSFˣ <span class="token operator">*</span> PSFʸ
    <span class="token keyword">end</span>
    <span class="token keyword">return</span> v
<span class="token keyword">end</span>
</code></pre>
<p>then found that introducing multithreading as follows significantly improves the performance.</p>
<pre class=" language-julia"><code class="prism  language-julia"><span class="token keyword">function</span> video_sim_v3<span class="token punctuation">(</span>xᵖ<span class="token punctuation">,</span> yᵖ<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    <span class="token number">F</span> <span class="token operator">=</span> size<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    v <span class="token operator">=</span> Array<span class="token punctuation">{</span>eltype<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">}</span><span class="token punctuation">(</span>undef<span class="token punctuation">,</span> length<span class="token punctuation">(</span>xᵖ<span class="token punctuation">)</span><span class="token punctuation">,</span> length<span class="token punctuation">(</span>yᵖ<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">F</span><span class="token punctuation">)</span>
    Threads<span class="token punctuation">.</span>@threads <span class="token keyword">for</span> <span class="token number">f</span> in <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">F</span>
        PSFˣ <span class="token operator">=</span> exp<span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>xᵖ <span class="token punctuation">.</span><span class="token operator">-</span> Transpose<span class="token punctuation">(</span>view<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">f</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span>
        PSFʸ <span class="token operator">=</span> exp<span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>view<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">f</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">-</span> Transpose<span class="token punctuation">(</span>yᵖ<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span>
        v<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">f</span><span class="token punctuation">]</span> <span class="token operator">=</span> PSFˣ <span class="token operator">*</span> PSFʸ
    <span class="token keyword">end</span>
    <span class="token keyword">return</span> v
<span class="token keyword">end</span>
</code></pre>
<p>Eventually, <code>video_sim_v3</code> yields a benchmark of <code>12.925 ms (1450 allocations: 123.47 MiB)</code> on my eight-thread Intel i7 7700K.</p>
<p>In the part II of my blog series, I have also loosely alluded to the dilemma we face in further optimization:</p>
<ul>
<li>The number of independent frames can be much larger than the number of threads on a CPU<sup class="footnote-ref"><a href="#fn2" id="fnref2">2</a></sup>.</li>
<li>Multiprocessing on a cluster causes significant communication overhead and development challenge, ultimately outweigh the potential performance gain.</li>
</ul>
<p>Basically, we want a solution that can efficiently execute numerous relatively lightweight computational tasks in parallel, while maintaining minimal communication overhead. Interestingly, such a solution already exists, and it takes the form of a GPU. According to the experts from <a href="https://www.intel.com/content/www/us/en/products/docs/processors/cpu-vs-gpu.html">Intel</a>,</p>
<blockquote>
<p>The GPU is a processor that is made up of many smaller and more specialized cores. By working together, the cores deliver massive performance when a processing task can be divided up and processed across many cores.</p>
</blockquote>
<h2 id="run-code-on-a-gpu">Run code on a GPU</h2>
<p>Originally popularized in the deep learning community, accelerating scientific computations with GPUs is rapidly getting attentions from researchers across various domains. Many thanks to the continuous efforts from scientists and software developers, writing GPU codes has become much easier than it used to be. Under some circumstances, once properly set up, running a code originally written for CPUs on GPUs can be achieved via merely changing a few lines.</p>
<p>At the moment, the three leading companies in chips, Nvidia, AMD, and Intel, all offer their own platforms for GPU computation<sup class="footnote-ref"><a href="#fn3" id="fnref3">3</a></sup>. Due to the relatively higher popularity, I will use <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a> from Nvidia in this blog. For detailed guidance on installation and integration with Julia, please refer to <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> and <a href="https://cuda.juliagpu.org/stable/">its documentation</a>.</p>
<h3 id="first-attempt">First attempt</h3>
<p>Once the installation of <code>CUDA.jl</code> is completed, verified, and loaded, to run <code>video_sim_v1</code> on an Nvidia GPU we simply need to pass arguments as CUDA arrays such as <code>video_sim_v1(CuArray(xᵖ), CuArray(yᵖ), CuArray(x), CuArray(y))</code>.</p>
<p>You may expect magic to happen but a warning (or sometimes an error) pops up regarding <code>performing scalar indexing on task</code>. What’s more, the warning message also says <code>such implementations *do not* execute on the GPU, but very slowly on the CPU</code>, indicating our first attempt has failed. The cause behind this failure is clear from the warning message: CUDA does not accept scalar indexing of a GPU array, like <code>v[:, :, f]</code>. Consequently, the solution entails a complete vectorization of the code, eliminating the need for the for-loop iteration over <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathnormal" style="margin-right: 0.10764em;">f</span></span></span></span></span>.</p>
<h3 id="further-vectorization">Further vectorization</h3>
<p>As stated multiple times thus far, our problem does not align directly with any basic vector operation. However, we can be clever and slightly restructure our data, enabling the potential for vectorization. An approach to achieve this is illustrated in the following figure.</p>
<p align="center" height="100%">
    <img src="https://github.com/lanceXwq/BlogPostFiles/blob/main/230808%20Code%20Optimization%20III/fig1.png?raw=true">
</p>
<p>Here, <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>S</mi><msup><mi>F</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">PSF^x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathnormal" style="margin-right: 0.05764em;">PS</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.13889em;">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>S</mi><msup><mi>F</mi><mi>y</mi></msup></mrow><annotation encoding="application/x-tex">PSF^y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathnormal" style="margin-right: 0.05764em;">PS</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.13889em;">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.03588em;">y</span></span></span></span></span></span></span></span></span></span></span></span>, and <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathnormal" style="margin-right: 0.22222em;">V</span></span></span></span></span> are restructured as block-diagonal matrices. Blocks sharing the same color correspond to the same frame, while any remaining elements within these matrices are set to zero, visually represented as white-colored sections. As a result, all the frames can be simulated through one matrix multiplication.</p>
<p>While this approach is indeed valid, I would not recommend implementing it by yourselves. This is due to the potentially vast dimensions of these block matrices. A naive implementation lacking efficient memory allocation handling could greatly worsen overall performance.</p>
<p>Are there better solutions? The answer is yes. This problem we are facing, namely numerous independent (and typically small) matrix multiplications of identical sizes, is not unique to us. In fact, it is common enough that people have named it “batch matrix multiplication”.</p>
<h3 id="batch-matrix-multiplication">Batch matrix multiplication</h3>
<p align="center" height="100%">
    <img src="https://github.com/lanceXwq/BlogPostFiles/blob/main/230808%20Code%20Optimization%20III/fig2.png?raw=true">
</p>
<p>Although batch matrix multiplication is widely recognized and efficiently implemented, it may not always be easy to find the correct function within your programming language. Occasionally, batch matrix multiplication goes by different names. For instance, in MATLAB, it is referred to as “<a href="https://www.mathworks.com/help/matlab/ref/pagemtimes.html">page-wise matrix multiplication</a>”. In certain cases, additional packages are required, and quite often, these packages belong deep-learning libraries! In Python, you can call <code>torch.bmm</code> from <a href="https://pytorch.org/docs/stable/generated/torch.bmm.html">PyTorch</a>, while Julia offers <code>batched_mul</code> through <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.batched_mul">Flux.jl</a>. Using <code>batched_mul</code>, we can write a new code as follows:</p>
<pre class=" language-julia"><code class="prism  language-julia"><span class="token keyword">function</span> video_sim_GPU_v2<span class="token punctuation">(</span>xᵖ<span class="token punctuation">,</span> yᵖ<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    PSFˣ <span class="token operator">=</span> exp<span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> size<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">-</span> xᵖ<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span>
    PSFʸ <span class="token operator">=</span> exp<span class="token punctuation">.</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>reshape<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> size<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">-</span> yᵖ<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> batched_mul<span class="token punctuation">(</span>PSFˣ<span class="token punctuation">,</span> batched_adjoint<span class="token punctuation">(</span>PSFʸ<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">end</span>
</code></pre>
<p>Here, <code>reshape</code> is called to construct <code>PSFˣ</code> and <code>PSFˣ</code> as 3D arrays, and <code>batched_adjoint</code> is just the “batched” version of transpose.</p>
<p>Benchmarking <code>video_sim_GPU_v2</code> on my CPU (i7 7700K) and my GPU (GeForce GTX 1060) yield <code>9.550 ms (75 allocations: 73.44 MiB)</code> and <code>3.127 ms (9 GPU allocations: 73.468 MiB</code><sup class="footnote-ref"><a href="#fn4" id="fnref4">4</a></sup>, respectively. Both of them are beating the multithreaded <code>video_sim_v3</code>!</p>
<h3 id="final-boost">Final boost</h3>
<p>The benchmarks I’ve showcased so far are based on double-precision float-point (float64) numbers. However, GPUs are frequently optimized for single-precision float-point (float32) numbers. For instance, once switched to using float32, <code>video_sim_GPU_v2</code>'s benchmark becomes <code>660.627 μs (11 GPU allocations: 36.736 MiB)</code>, another fivefold acceleration!</p>
<p>Therefore, it is frequently advantageous to craft your GPU code to support both float64 and float32, and then assess whether altering the datatype affects your outcome. If there’s no impact, simply proceed with float32!</p>
<h2 id="conclusion">Conclusion</h2>
<p>Finally, we have arrived at the conclusion of my blog series concerning optimization techniques for scientific computation. I hope you have enjoyed this journey and learned something useful. Please feel free to get in touch with me should you wish to connect or share your thoughts!</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>In case you haven’t read the preceding blogs, I strongly encourage you to take a moment to review their problem description sections. This will provide you with a better picture of the issue I’m trying to address. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>As of the date of this blog, even the most advanced desktop CPU, AMD Ryzen™ Threadripper™ PRO 5995WX (~$6,000), only has 128 threads, while frame number can easily be over 1,000. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a> from Nvidia, <a href="https://en.wikipedia.org/wiki/ROCm">ROCm</a> from AMD, and <a href="https://en.wikipedia.org/wiki/OneAPI_(compute_acceleration)">OneAPI</a> from Intel. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>GPU memory allocation is measured by <code>CUDA.@time</code>, see this <a href="https://cuda.juliagpu.org/stable/development/profiling/">page</a>. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</div>
</body>

</html>
